# streamlit_RAG.py
# Interface Streamlit pour :
#  - ingestion de documents dans FAISS (via CSV ou upload direct)
#  - question / r√©ponse RAG via Snowflake + DALLEM
#
# Cette version :
#  - s√©pare clairement l‚Äôonglet d‚Äôingestion et l‚Äôonglet RAG
#  - le bouton "Lancer l'ingestion" n‚Äôappara√Æt QUE dans l‚Äôonglet Ingestion

import os
import tempfile
import shutil
import hashlib
import shutil
import csv
import io
import getpass
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Callable, Optional

import streamlit as st

from models_utils import EMBED_MODEL, LLM_MODEL, make_logger
from rag_ingestion import ingest_documents, build_faiss_store
from rag_query import run_rag_query
from feedback_store import FeedbackStore, create_feedback, QueryFeedback
from config_manager import (
    load_config,
    save_config,
    is_config_valid,
    render_config_page_streamlit,
    StorageConfig,
    validate_all_directories,
    create_directory,
)
from xml_processing import (
    XMLParseConfig,
    SectionPattern,
    PATTERN_DESCRIPTIONS,
    analyze_xml,
    preview_sections,
)

# Optionnel : fonction d'extraction des pi√®ces jointes PDF si disponible
try:
    from pdf_processing import extract_attachments_from_pdf
except ImportError:
    extract_attachments_from_pdf = None

logger = make_logger(debug=False)


# =====================================================================
#  CACHE POUR LES RESSOURCES ET REQU√äTES RAG (am√©lioration performance)
# =====================================================================

@st.cache_resource(ttl=600, show_spinner=False)  # Cache 10 minutes (singleton)
def get_cached_faiss_store(db_path: str):
    """Retourne un store FAISS cach√© pour √©viter les reconstructions."""
    return build_faiss_store(db_path)


@st.cache_data(ttl=1800, show_spinner=False)  # Cache 30 minutes
def cached_rag_query(
    db_path: str,
    collection_name: str,
    question: str,
    top_k: int = 30,
    synthesize_all: bool = False,
    use_feedback_reranking: bool = False,
    use_query_expansion: bool = True,
    use_bge_reranker: bool = True,
) -> dict:
    """
    Version cach√©e de run_rag_query pour √©viter les recalculs sur requ√™tes identiques.
    Le cache est invalid√© apr√®s 30 minutes (ttl=1800).
    """
    return run_rag_query(
        db_path=db_path,
        collection_name=collection_name,
        question=question,
        top_k=top_k,
        synthesize_all=synthesize_all,
        log=logger,
        feedback_store=None,  # Pas de feedback dans le cache (sinon probl√®mes de s√©rialisation)
        use_feedback_reranking=use_feedback_reranking,
        use_query_expansion=use_query_expansion,
        use_bge_reranker=use_bge_reranker,
    )


# =====================================================================
#  CONFIG LOCALE - CHARG√âE DEPUIS config.json
# =====================================================================
# La configuration est maintenant g√©r√©e par config_manager.py
# Si les r√©pertoires ne sont pas accessibles, une page de configuration
# sera affich√©e pour permettre √† l'utilisateur de les configurer.

# Charger la configuration
_config = load_config()

# Variables globales pour compatibilit√© avec le code existant
BASE_ROOT_DIR = _config.base_root_dir
CSV_IMPORT_DIR = _config.csv_import_dir
CSV_EXPORT_DIR = _config.csv_export_dir
FEEDBACK_DIR = _config.feedback_dir


def _init_feedback_store():
    """Initialise le store de feedbacks de mani√®re s√©curis√©e."""
    try:
        if os.path.exists(FEEDBACK_DIR) or os.access(os.path.dirname(FEEDBACK_DIR), os.W_OK):
            os.makedirs(FEEDBACK_DIR, exist_ok=True)
            return FeedbackStore(FEEDBACK_DIR)
    except Exception as e:
        logger.warning(f"[CONFIG] Impossible d'initialiser le feedback store: {e}")
    return None


# Initialisation du store de feedbacks (peut √™tre None si r√©pertoire inaccessible)
feedback_store = _init_feedback_store()



def build_attachment_csv_path(source_path: str, attachment_filename: str) -> str:
    """
    Construit le chemin logique pour une pi√®ce jointe √† enregistrer dans le CSV.
    Format : dir(source) / (stem(source) + "-" + attachment_filename)
    Exemple :
        source_path = D:\test\toto.pdf
        attachment_filename = tata.pdf
        -> D:\test\toto-tata.pdf
    """
    directory = os.path.dirname(source_path)
    stem, _ = os.path.splitext(os.path.basename(source_path))
    return os.path.join(directory, f"{stem}-{attachment_filename}")


def get_tracking_csv_path(base_name: str) -> str:
    """
    Retourne le chemin du fichier CSV de tracking pour une base donn√©e.
    Format : documents_ingeres_[nom_base].csv
    """
    if not CSV_EXPORT_DIR or CSV_EXPORT_DIR == "A_COMPLETER_CHEMIN_EXPORT_CSV":
        return None
    csv_filename = f"documents_ingeres_{base_name}.csv"
    return os.path.join(CSV_EXPORT_DIR, csv_filename)

# ========================
#   Gestion des verrous d'ingestion (coordination multi-utilisateurs)
# ========================

def create_ingestion_lock(base_root: str, base_name: str) -> bool:
    """
    Cr√©e un fichier de verrou pour indiquer qu'une ingestion est en cours.
    Retourne True si le verrou a √©t√© cr√©√©, False si la base est d√©j√† verrouill√©e.
    """
    import datetime
    import socket

    lock_dir = os.path.join(base_root, base_name)
    os.makedirs(lock_dir, exist_ok=True)

    lock_file = os.path.join(lock_dir, ".ingestion_lock")

    # V√©rifier si le verrou existe d√©j√†
    if os.path.exists(lock_file):
        return False

    # Cr√©er le fichier de verrou avec des infos
    try:
        with open(lock_file, "w", encoding="utf-8") as f:
            f.write(f"timestamp={datetime.datetime.now().isoformat()}\n")
            f.write(f"hostname={socket.gethostname()}\n")
            f.write(f"user={os.environ.get('USERNAME', os.environ.get('USER', 'unknown'))}\n")
        return True
    except Exception as e:
        logger.error(f"Erreur lors de la cr√©ation du verrou : {e}")
        return False


def remove_ingestion_lock(base_root: str, base_name: str) -> None:
    """Supprime le fichier de verrou d'ingestion."""
    lock_file = os.path.join(base_root, base_name, ".ingestion_lock")
    try:
        if os.path.exists(lock_file):
            os.remove(lock_file)
            logger.info(f"Verrou d'ingestion supprim√© pour {base_name}")
    except Exception as e:
        logger.error(f"Erreur lors de la suppression du verrou : {e}")


def is_base_locked(base_root: str, base_name: str, timeout_hours: int = 2) -> bool:
    """
    V√©rifie si une base est verrouill√©e pour ingestion.
    Si le verrou est plus vieux que timeout_hours, il est consid√©r√© comme orphelin et supprim√©.
    """
    import datetime

    lock_file = os.path.join(base_root, base_name, ".ingestion_lock")

    if not os.path.exists(lock_file):
        return False

    # V√©rifier l'√¢ge du verrou
    try:
        mod_time = datetime.datetime.fromtimestamp(os.path.getmtime(lock_file))
        age = datetime.datetime.now() - mod_time

        if age.total_seconds() > timeout_hours * 3600:
            logger.warning(f"Verrou orphelin d√©tect√© pour {base_name} (√¢ge: {age}). Suppression automatique.")
            remove_ingestion_lock(base_root, base_name)
            return False

        return True
    except Exception as e:
        logger.error(f"Erreur lors de la v√©rification du verrou : {e}")
        return False


def get_lock_info(base_root: str, base_name: str) -> Optional[Dict[str, str]]:
    """Retourne les informations du verrou (timestamp, user, hostname) ou None."""
    import datetime

    lock_file = os.path.join(base_root, base_name, ".ingestion_lock")

    if not os.path.exists(lock_file):
        return None

    try:
        info = {}
        with open(lock_file, "r", encoding="utf-8") as f:
            for line in f:
                if "=" in line:
                    key, value = line.strip().split("=", 1)
                    info[key] = value

        # Calculer la dur√©e
        if "timestamp" in info:
            lock_time = datetime.datetime.fromisoformat(info["timestamp"])
            duration = datetime.datetime.now() - lock_time
            info["duration"] = str(duration).split('.')[0]  # Format HH:MM:SS

        return info
    except Exception as e:
        logger.error(f"Erreur lors de la lecture des infos du verrou : {e}")
        return None


# ========================
#   Utils CSV / bases / collections
# ========================

@st.cache_data(ttl=300, show_spinner=False)  # Cache 5 minutes
def list_bases(base_root: str) -> List[str]:
    """Liste les bases FAISS (sous-dossiers) dans base_root."""
    p = Path(base_root)
    if not p.exists() or not p.is_dir():
        return []
    return sorted([d.name for d in p.iterdir() if d.is_dir()])


@st.cache_data(ttl=300, show_spinner=False)  # Cache 5 minutes
def list_collections_for_base(base_root: str, base_name: str) -> List[str]:
    """Liste les collections d'une base donn√©e (en interrogeant FAISS)."""
    db_path = os.path.join(base_root, base_name)
    try:
        store = get_cached_faiss_store(db_path)
        colls = store.list_collections()  # FAISS retourne directement une liste de noms
        return sorted(colls)
    except Exception:
        return []


@st.cache_data(ttl=300, show_spinner=False)  # Cache 5 minutes
def get_collection_doc_counts(base_root: str, base_name: str) -> Dict[str, int]:
    """
    Retourne un dict {nom_collection: nombre_d'√©l√©ments} pour une base donn√©e.
    Le nombre correspond au nombre de chunks index√©s dans la collection.
    """
    db_path = os.path.join(base_root, base_name)
    counts: Dict[str, int] = {}
    try:
        store = get_cached_faiss_store(db_path)
        for col_name in store.list_collections():  # FAISS retourne directement les noms
            try:
                collection = store.get_collection(col_name)
                n = collection.count()
            except Exception:
                n = 0
            counts[col_name] = n
    except Exception as e:
        logger.warning(f"[UI] Impossible de compter les documents pour base={base_name} : {e}")
    return counts


def parse_csv_groups_and_paths(file_bytes: bytes) -> Dict[str, List[str]]:
    """Parse un CSV bytes -> dict {group: [paths...]}.

    - s√©parateur ';' ou ',' (d√©tection automatique)
    - en-t√™te possible : group;path
    - sinon, col0 = group, col1 = path
    - nettoyage du BOM √©ventuel sur 'group'
    """
    # D√©coder les bytes en texte, g√©rer BOM UTF-8
    try:
        text = file_bytes.decode("utf-8-sig", errors="ignore")
    except Exception:
        text = file_bytes.decode("utf-8", errors="ignore")

    # Normaliser les sauts de ligne (Windows \r\n -> \n)
    text = text.replace('\r\n', '\n').replace('\r', '\n')

    # Cr√©er le buffer StringIO
    buf = io.StringIO(text)

    # D√©tecter le d√©limiteur
    sample = text[:2048]
    try:
        dialect = csv.Sniffer().sniff(sample, delimiters=";,")
        delimiter = dialect.delimiter
    except Exception:
        delimiter = ";"

    reader = csv.reader(buf, delimiter=delimiter)
    rows = list(reader)

    if not rows:
        return {}

    header = rows[0]
    header = [h.lstrip("\ufeff") for h in header]
    header_lower = [h.strip().lower() for h in header]

    if "group" in header_lower and "path" in header_lower:
        idx_group = header_lower.index("group")
        idx_path = header_lower.index("path")
        data_rows = rows[1:]
    else:
        idx_group = 0
        idx_path = 1 if len(header) > 1 else 0
        data_rows = rows

    groups: Dict[str, List[str]] = {}
    for r in data_rows:
        if len(r) <= max(idx_group, idx_path):
            continue
        g = (r[idx_group] or "").strip().lstrip("\ufeff")
        p = (r[idx_path] or "").strip()
        if not p:
            continue
        if not g:
            g = "ALL"
        groups.setdefault(g, []).append(p)

    return groups


# ========================
#   PAGE CONFIG
# ========================

st.set_page_config(
    page_title="RaGME_UP - PROP",
    layout="wide",
)

# ========================
#   V√âRIFICATION DE LA CONFIGURATION
# ========================
# Si les r√©pertoires ne sont pas accessibles, afficher la page de configuration

if not is_config_valid(_config):
    st.title("Configuration des r√©pertoires")

    st.warning("Les r√©pertoires de stockage ne sont pas accessibles. Veuillez les configurer.")

    # Afficher le statut de chaque r√©pertoire
    results = validate_all_directories(_config)

    col1, col2 = st.columns([2, 1])

    with col1:
        st.subheader("Statut des r√©pertoires")

        for key, (valid, message, label, path) in results.items():
            if valid:
                st.success(f"‚úÖ **{label}**\n\n`{path}`")
            else:
                st.error(f"‚ùå **{label}**\n\n`{path}`\n\n_{message}_")

    with col2:
        st.subheader("Actions")

        # Bouton pour cr√©er les r√©pertoires manquants
        if st.button("Cr√©er les r√©pertoires manquants", type="primary"):
            created = []
            failed = []

            for key, (valid, message, label, path) in results.items():
                if not valid:
                    success, msg = create_directory(path)
                    if success:
                        created.append(label)
                    else:
                        failed.append(f"{label}: {msg}")

            if created:
                st.success(f"Cr√©√©s: {', '.join(created)}")
            if failed:
                st.error(f"√âchecs: {', '.join(failed)}")

            st.rerun()

        st.markdown("---")

        # Formulaire pour modifier les chemins
        st.subheader("Modifier les chemins")

        new_base_root = st.text_input("Bases FAISS", value=_config.base_root_dir)
        new_csv_import = st.text_input("CSV ingestion", value=_config.csv_import_dir)
        new_csv_export = st.text_input("CSV tracking", value=_config.csv_export_dir)
        new_feedback = st.text_input("Feedbacks", value=_config.feedback_dir)

        if st.button("Sauvegarder la configuration"):
            new_config = StorageConfig(
                base_root_dir=new_base_root,
                csv_import_dir=new_csv_import,
                csv_export_dir=new_csv_export,
                feedback_dir=new_feedback,
            )
            if save_config(new_config):
                st.success("Configuration sauvegard√©e!")
                st.rerun()
            else:
                st.error("Erreur lors de la sauvegarde")

    st.info("""
    **Aide:**
    - Les chemins doivent √™tre des chemins absolus
    - Les r√©pertoires doivent √™tre accessibles en lecture et √©criture
    - Cliquez sur "Cr√©er les r√©pertoires manquants" pour cr√©er les dossiers automatiquement
    - Ou modifiez les chemins et cliquez sur "Sauvegarder la configuration"
    """)

    st.stop()  # Arr√™ter l'ex√©cution du reste de l'app

# ========================
#   APPLICATION PRINCIPALE
# ========================

st.title("RaGME_UP - PROP")

# ========================
#   GUIDE UTILISATEUR
# ========================
with st.expander("üìñ **GUIDE UTILISATEUR** - Cliquez pour afficher l'aide", expanded=False):
    try:
        with open("GUIDE_UTILISATEUR.md", "r", encoding="utf-8") as f:
            guide_content = f.read()
        st.markdown(guide_content)
    except FileNotFoundError:
        st.error("‚ùå Le fichier GUIDE_UTILISATEUR.md est introuvable.")
    except Exception as e:
        st.error(f"‚ùå Erreur lors du chargement du guide : {e}")

# ========================
#   SIDEBAR
# ========================

# V√©rifier l'utilisateur pour afficher la sidebar
current_user = getpass.getuser().lower()
allowed_users = ["agdgtrl", "renau"]

# D√©finir les variables par d√©faut (utilis√©es m√™me si sidebar cach√©e)
base_root = BASE_ROOT_DIR
bases = list_bases(base_root)
use_easa_sections = st.session_state.get("use_easa_sections", False)

if current_user in allowed_users:
    with st.sidebar:
        st.header("‚öôÔ∏è Configuration globale")

        base_root = BASE_ROOT_DIR
        st.warning(
            f"üìÅ Bases FAISS : `{BASE_ROOT_DIR}`"
        )
        st.warning(
            f"üìù CSV d'ingestion : `{CSV_IMPORT_DIR}`"
        )
        st.warning(
            f"üìä CSV de tracking : `{CSV_EXPORT_DIR}`"
        )
        bases = list_bases(base_root)
        if bases:
            st.success(f"‚úÖ {len(bases)} base(s) trouv√©e(s) sous {base_root}")
        else:
            st.info("‚ÑπÔ∏è Aucune base trouv√©e sous ce dossier pour l'instant.")

        st.markdown("---")

        st.markdown("### ü§ñ Mod√®les utilis√©s")
        st.caption(f"üîπ Embeddings : **Snowflake** ‚Äì `{EMBED_MODEL}`")
        st.caption(f"üîπ LLM : **DALLEM** ‚Äì `{LLM_MODEL}`")


# ========================
#   TABS
# ========================
# Liste des utilisateurs autoris√©s √† voir le tableau de bord analytique
ANALYTICS_AUTHORIZED_USERS = ["agdgtrl", "renau"]
current_user = getpass.getuser().lower()

# Cr√©er les tabs conditionnellement
if current_user in ANALYTICS_AUTHORIZED_USERS:
    tab_csv, tab_ingest, tab_purge, tab_rag, tab_analytics = st.tabs(
        ["üìù Gestion CSV", "üì• Ingestion documents", "üóëÔ∏è Purge des bases", "‚ùì Questions RAG", "üìä Tableau de bord"]
    )
else:
    tab_csv, tab_ingest, tab_purge, tab_rag = st.tabs(
        ["üìù Gestion CSV", "üì• Ingestion documents", "üóëÔ∏è Purge des bases", "‚ùì Questions RAG"]
    )
    tab_analytics = None  # Pas d'acc√®s au tableau de bord


# ========================
#   TAB GESTION CSV
# ========================
with tab_csv:
    st.subheader("üìù Gestion des fichiers CSV d'ingestion")

    # Initialiser le mode dans session_state
    if "csv_mode" not in st.session_state:
        st.session_state.csv_mode = None

    # Deux gros boutons pour choisir le mode
    st.markdown("### Choisissez une action")
    col_btn1, col_btn2 = st.columns(2)

    with col_btn1:
        if st.button(
            "üìù Cr√©ation d'un CSV",
            type="primary",
            use_container_width=True,
            help="Lance l'interface GUI moderne pour cr√©er un CSV avec vrais chemins"
        ):
            # Lancer directement l'application GUI
            import subprocess
            import sys

            try:
                if sys.platform == "win32":
                    subprocess.Popen(
                        ["python", "csv_generator_gui.py"],
                        creationflags=subprocess.CREATE_NEW_CONSOLE
                    )
                else:
                    subprocess.Popen(["python", "csv_generator_gui.py"])

                st.success("‚úÖ Application GUI lanc√©e ! V√©rifiez vos fen√™tres ouvertes.")
                st.info("üí° Utilisez l'application pour cr√©er votre CSV, puis revenez ici pour l'ing√©rer.")
            except Exception as e:
                st.error(f"‚ùå Erreur lors du lancement : {e}")
                st.info("üí° Lancez manuellement dans un terminal :")
                st.code("python csv_generator_gui.py", language="bash")

    with col_btn2:
        if st.button(
            "üìù √âditer un CSV",
            type="primary",
            use_container_width=True,
            help="Lance l'interface GUI pour √©diter un CSV existant"
        ):
            # Lancer la GUI sans argument - l'utilisateur choisira le CSV dans l'interface
            import subprocess
            import sys

            try:
                if sys.platform == "win32":
                    subprocess.Popen(
                        ["python", "csv_generator_gui.py"],
                        creationflags=subprocess.CREATE_NEW_CONSOLE
                    )
                else:
                    subprocess.Popen(["python", "csv_generator_gui.py"])

                st.success("‚úÖ Application GUI lanc√©e !")
                st.info("üí° Utilisez le bouton 'üìÇ Ouvrir un CSV' dans l'interface pour charger un CSV existant.")
            except Exception as e:
                st.error(f"‚ùå Erreur lors du lancement : {e}")
                st.info("üí° Lancez manuellement dans un terminal :")
                st.code("python csv_generator_gui.py", language="bash")

    st.markdown("---")


# ========================
#   TAB INGESTION
# ========================
with tab_ingest:
    st.subheader("üì• Ingestion de documents via CSV")

    chunk_size = 1000

    # Initialiser le flag d'arr√™t d'ingestion
    if "stop_ingestion" not in st.session_state:
        st.session_state["stop_ingestion"] = False
    if "ingestion_running" not in st.session_state:
        st.session_state["ingestion_running"] = False
    if "created_locks" not in st.session_state:
        st.session_state["created_locks"] = []

    # Nettoyage des verrous si l'ingestion a √©t√© interrompue
    if st.session_state["stop_ingestion"] and st.session_state["created_locks"]:
        for base_name in st.session_state["created_locks"]:
            remove_ingestion_lock(base_root, base_name)
            logger.info(f"Verrou d'ingestion nettoy√© apr√®s arr√™t pour {base_name}")
        st.session_state["created_locks"] = []
        st.session_state["stop_ingestion"] = False
        st.session_state["ingestion_running"] = False
        st.rerun()

    # Option EASA sections
    if "use_easa_sections" not in st.session_state:
        st.session_state["use_easa_sections"] = False

    use_easa_sections = st.checkbox(
        "Utiliser les sections EASA (CS / AMC / GM)",
        value=st.session_state["use_easa_sections"],
        help=(
            "Quand activ√©, le texte est d'abord d√©coup√© par sections CS/AMC/GM "
            "avant le chunking."
        ),
    )
    st.session_state["use_easa_sections"] = use_easa_sections

    st.markdown("---")

    # S√©lection de CSV depuis CSV_IMPORT_DIR
    st.markdown(f"### üìÇ S√©lectionner des CSV depuis `{CSV_IMPORT_DIR}`")

    # Lister les fichiers CSV disponibles dans CSV_IMPORT_DIR
    available_csvs = []
    if CSV_IMPORT_DIR and os.path.isdir(CSV_IMPORT_DIR):
        available_csvs = sorted([
            f for f in os.listdir(CSV_IMPORT_DIR)
            if f.lower().endswith('.csv')
        ])

    if available_csvs:
        selected_csv_files = st.multiselect(
            "Fichiers CSV disponibles",
            options=available_csvs,
            help=f"S√©lectionnez un ou plusieurs CSV depuis {CSV_IMPORT_DIR}"
        )
    else:
        selected_csv_files = []
        if not CSV_IMPORT_DIR:
            st.warning("‚ö†Ô∏è CSV_IMPORT_DIR non configur√©")
        elif not os.path.isdir(CSV_IMPORT_DIR):
            st.warning(f"‚ö†Ô∏è R√©pertoire inexistant : {CSV_IMPORT_DIR}")
        else:
            st.info("Aucun fichier CSV trouv√© dans le r√©pertoire")

    st.markdown("---")
    st.markdown("### üì§ Ou uploader des CSV")

    uploaded_csvs = st.file_uploader(
        "Upload CSV",
        type=["csv"],
        accept_multiple_files=True,
        key="csv_ingest",
        label_visibility="collapsed"
    )

    # Combiner les CSV s√©lectionn√©s et upload√©s
    csv_files_to_process = []

    # Ajouter les CSV s√©lectionn√©s depuis le r√©pertoire
    for csv_name in selected_csv_files:
        csv_path = os.path.join(CSV_IMPORT_DIR, csv_name)
        if os.path.exists(csv_path):
            csv_files_to_process.append(("local", csv_name, csv_path))

    # Ajouter les CSV upload√©s
    if uploaded_csvs:
        for uploaded in uploaded_csvs:
            csv_files_to_process.append(("uploaded", uploaded.name, uploaded))

    # Afficher l'√©tat des bases (indicateur de disponibilit√© pour coordination)
    st.markdown("---")
    st.markdown("### üìä √âtat des bases (coordination multi-utilisateurs)")

    if bases:
        status_cols = st.columns(min(len(bases), 4))  # Max 4 colonnes
        for idx, base in enumerate(bases):
            col_idx = idx % 4
            with status_cols[col_idx]:
                is_locked = is_base_locked(base_root, base)
                if is_locked:
                    lock_info = get_lock_info(base_root, base)
                    user = lock_info.get("user", "unknown") if lock_info else "unknown"
                    duration = lock_info.get("duration", "?") if lock_info else "?"
                    st.error(f"üî¥ **{base}**")
                    st.caption(f"Ingestion en cours\nUtilisateur: {user}\nDur√©e: {duration}")
                else:
                    st.success(f"üü¢ **{base}**")
                    st.caption("Disponible")
    else:
        st.info("Aucune base trouv√©e")

    st.markdown("---")

    # Pour reconstruire un CSV r√©capitulatif : base;group;path
    ingested_entries: List[Tuple[str, str, str]] = []
    logical_paths: Dict[str, str] = {}

    # Statistiques d'ingestion
    ingestion_stats = {
        "csv_new_files": 0,
        "csv_missing_files": 0,
        "csv_skipped_existing": 0,
        "csv_attachments": 0,
    }

    # R√©capitulatif des fichiers ing√©r√©s pendant cette ex√©cution
    session_ingested_files = []

    # ========================================================================
    # D√âTECTION ET CONFIGURATION DES FICHIERS XML
    # ========================================================================

    # Initialiser les √©tats de session pour XML
    if "xml_configs" not in st.session_state:
        st.session_state["xml_configs"] = {}  # {path: XMLParseConfig}
    if "xml_preview_validated" not in st.session_state:
        st.session_state["xml_preview_validated"] = False
    if "detected_xml_files" not in st.session_state:
        st.session_state["detected_xml_files"] = []

    def detect_xml_files_in_csvs(csv_list) -> List[str]:
        """Analyse les CSV (locaux ou upload√©s) pour trouver les fichiers XML."""
        xml_files = []
        for source_type, csv_name, csv_source in csv_list:
            # Lire le contenu selon la source
            if source_type == "local":
                with open(csv_source, "rb") as f:
                    content = f.read()
            else:
                csv_source.seek(0)
                content = csv_source.read()
                csv_source.seek(0)
            groups = parse_csv_groups_and_paths(content)
            for group_name, paths in groups.items():
                for path in paths:
                    if path.lower().endswith(".xml") and os.path.isfile(path):
                        if path not in xml_files:
                            xml_files.append(path)
        return xml_files

    # D√©tecter les fichiers XML quand des CSV sont s√©lectionn√©s/upload√©s
    xml_files_detected = []
    if csv_files_to_process:
        xml_files_detected = detect_xml_files_in_csvs(csv_files_to_process)
        st.session_state["detected_xml_files"] = xml_files_detected

    # Afficher l'interface de pr√©visualisation XML si des fichiers XML sont d√©tect√©s
    if xml_files_detected:
        st.markdown("---")
        st.markdown("### üìÑ Fichiers XML d√©tect√©s - Normes EASA")
        st.info(f"**{len(xml_files_detected)} fichier(s) XML** d√©tect√©(s). "
                "Choisissez le pattern de d√©coupage pour chaque fichier.")

        for xml_path in xml_files_detected:
            with st.expander(f"üìÑ {os.path.basename(xml_path)}", expanded=True):
                try:
                    # S√©lection du pattern de d√©coupage
                    pattern_options = list(SectionPattern)
                    pattern_labels = [PATTERN_DESCRIPTIONS[p] for p in pattern_options]

                    selected_idx = st.selectbox(
                        "Pattern de d√©coupage",
                        range(len(pattern_options)),
                        index=pattern_options.index(SectionPattern.ALL_EASA),
                        format_func=lambda i: pattern_labels[i],
                        key=f"pattern_{xml_path}",
                        help="Choisissez comment d√©couper le document en sections"
                    )
                    selected_pattern = pattern_options[selected_idx]

                    # Champ pour pattern custom
                    custom_regex = None
                    if selected_pattern == SectionPattern.CUSTOM:
                        custom_regex = st.text_input(
                            "Pattern regex personnalis√©",
                            value=r"(SECTION[-\s]?\d+)",
                            key=f"custom_pattern_{xml_path}",
                            help="Entrez une expression r√©guli√®re. Le groupe (1) sera utilis√© comme code de section."
                        )
                        st.caption("Exemples: `(CHAPTER\\s+\\d+)`, `(ART\\.?\\s*\\d+)`, `(¬ß\\s*\\d+\\.\\d+)`")

                    # Cr√©er la config
                    config = XMLParseConfig(
                        pattern_type=selected_pattern,
                        custom_pattern=custom_regex
                    )

                    # Analyser avec le pattern s√©lectionn√©
                    analysis = analyze_xml(xml_path, config)

                    if analysis.get("error"):
                        st.error(f"Erreur: {analysis['error']}")
                        continue

                    # Stats
                    col1, col2 = st.columns(2)
                    with col1:
                        st.metric("üìä Caract√®res", f"{analysis['total_chars']:,}")
                    with col2:
                        st.metric("üìë Sections d√©tect√©es", analysis['sections_count'])

                    # Liste des sections trouv√©es
                    if analysis['sections']:
                        st.markdown("**Sections trouv√©es:**")
                        sections_display = []
                        for sec in analysis['sections'][:15]:
                            title_part = f" - {sec['title']}" if sec['title'] else ""
                            sections_display.append(f"‚Ä¢ **{sec['code']}**{title_part} ({sec['length']:,} car.)")

                        st.markdown("\n".join(sections_display))

                        if analysis['sections_count'] > 15:
                            st.caption(f"... et {analysis['sections_count'] - 15} autres sections")

                    # Pr√©visualisation du texte
                    if st.button(f"üëÅÔ∏è Voir aper√ßu complet", key=f"preview_{xml_path}"):
                        preview_text, _ = preview_sections(xml_path, config, max_sections=20)
                        st.text_area("Aper√ßu", preview_text, height=400, key=f"preview_area_{xml_path}")

                    # Sauvegarder la config
                    st.session_state["xml_configs"][xml_path] = config

                except Exception as e:
                    st.error(f"Erreur: {e}")

        # Validation
        if st.button("‚úÖ Confirmer et continuer", type="primary"):
            st.session_state["xml_preview_validated"] = True
            st.success("Configuration valid√©e !")

        if not st.session_state["xml_preview_validated"]:
            st.warning("‚ö†Ô∏è Cliquez sur 'Confirmer et continuer' pour valider.")

        st.markdown("---")

    # Bouton d'ingestion et bouton stop
    can_ingest = True
    if xml_files_detected and not st.session_state["xml_preview_validated"]:
        can_ingest = False

    # Callback pour le bouton stop
    def stop_ingestion_callback():
        st.session_state["stop_ingestion"] = True

    # Afficher les boutons c√¥te √† c√¥te
    col_start, col_stop = st.columns([3, 1])
    with col_start:
        start_button = st.button(
            "üöÄ Lancer l'ingestion",
            help="Lance l'ingestion des documents list√©s dans le CSV. Les fichiers sont d√©coup√©s en chunks, vectoris√©s et index√©s dans FAISS.",
            disabled=not can_ingest
        )
    with col_stop:
        # Placeholder pour le bouton stop (visible uniquement pendant l'ingestion)
        stop_button_placeholder = st.empty()

    if start_button:
        # R√©initialiser le flag d'arr√™t
        st.session_state["stop_ingestion"] = False
        st.session_state["ingestion_running"] = True
        # ------------------------------------------------------------------
        # Charger les CSV de tracking par base existants pour √©viter de r√©-ing√©rer
        # des fichiers d√©j√† trait√©s.
        # Cl√© utilis√©e : (base, group, path)
        # Structure : {base_name: {(group, path): True}}
        # ------------------------------------------------------------------
        existing_entries_by_base: Dict[str, Dict[Tuple[str, str], bool]] = {}

        def load_tracking_csv_for_base(base_name: str) -> Dict[Tuple[str, str], bool]:
            """Charge le CSV de tracking pour une base donn√©e."""
            entries = {}
            csv_path = get_tracking_csv_path(base_name)
            if csv_path and os.path.exists(csv_path):
                try:
                    with open(csv_path, "r", encoding="utf-8-sig", newline="") as f_csv:
                        reader = csv.reader(f_csv, delimiter=";")
                        rows = list(reader)
                        # Skip header if present
                        start_idx_rows = 1 if rows and rows[0] and rows[0][0].lower() == "group" else 0
                        for r in rows[start_idx_rows:]:
                            if len(r) < 2:
                                continue
                            g, p = r[0].strip(), r[1].strip()
                            # Stocker √† la fois le chemin original et normalis√© pour une comparaison robuste
                            entries[(g, p)] = True
                            p_normalized = os.path.normpath(p)
                            if p_normalized != p:
                                entries[(g, p_normalized)] = True
                except Exception as e:
                    st.error(f"Erreur lors de la lecture du CSV de tracking pour {base_name} : {e}")
            return entries

        # Rien √† ing√©rer ?
        if not csv_files_to_process:
            st.warning("Aucun CSV d'ingestion s√©lectionn√© ou upload√©.")
        else:
            # Collecter les bases qui vont √™tre modifi√©es (depuis les noms de CSV)
            bases_to_ingest = set()
            for source_type, csv_name, _ in csv_files_to_process:
                base_name = Path(csv_name).stem
                bases_to_ingest.add(base_name)

            # V√©rifier qu'aucune base n'est d√©j√† verrouill√©e
            locked_bases = []
            for base_name in bases_to_ingest:
                if is_base_locked(base_root, base_name):
                    locked_bases.append(base_name)

            if locked_bases:
                st.error(
                    f"‚ùå Impossible de lancer l'ingestion : les bases suivantes sont d√©j√† en cours d'ingestion :\n\n"
                    + "\n".join([f"- **{b}**" for b in locked_bases])
                    + "\n\nVeuillez attendre la fin de l'ingestion en cours ou v√©rifier les indicateurs ci-dessus."
                )
                st.stop()

            # Cr√©er les verrous pour toutes les bases
            created_locks = []
            st.session_state["created_locks"] = []  # R√©initialiser
            for base_name in bases_to_ingest:
                if create_ingestion_lock(base_root, base_name):
                    created_locks.append(base_name)
                    st.session_state["created_locks"].append(base_name)  # Sauvegarder pour cleanup
                    logger.info(f"Verrou d'ingestion cr√©√© pour {base_name}")
                else:
                    st.error(f"‚ùå Impossible de cr√©er le verrou pour la base {base_name}")
                    # Nettoyer les verrous d√©j√† cr√©√©s
                    for b in created_locks:
                        remove_ingestion_lock(base_root, b)
                    st.session_state["created_locks"] = []
                    st.stop()

            try:
                progress_bar = st.progress(0.0)
                status_text = st.empty()
                log_box = st.empty()
                log_lines: List[str] = []

                # Afficher le bouton stop
                stop_button_placeholder.button(
                    "üõë Stop",
                    type="secondary",
                    on_click=stop_ingestion_callback,
                    key="stop_ingestion_btn"
                )

                def check_stop() -> bool:
                    """V√©rifie si l'utilisateur a demand√© l'arr√™t."""
                    return st.session_state.get("stop_ingestion", False)

                def log(msg: str) -> None:
                    logger.info(msg)
                    log_lines.append(msg)
                    log_box.text_area("ü™µ Logs ingestion", value="\n".join(log_lines), height=280)

                def progress(frac: float, msg: str) -> None:
                    progress_bar.progress(min(max(frac, 0.0), 1.0))
                    status_text.info(msg)

                # ------------------------------------------------------------------
                # 1) Ingestion √† partir des CSV
                # ------------------------------------------------------------------
                ingestion_stopped = False
                if csv_files_to_process:
                    csv_temp_dir = tempfile.mkdtemp(prefix="rag_ingest_csv_")
                    try:
                        for source_type, csv_name, csv_source in csv_files_to_process:
                            # V√©rifier si l'utilisateur a demand√© l'arr√™t
                            if check_stop():
                                log("‚ö†Ô∏è Ingestion interrompue par l'utilisateur")
                                ingestion_stopped = True
                                break
                            # Lire le contenu selon la source (locale ou upload√©e)
                            if source_type == "local":
                                with open(csv_source, "rb") as f:
                                    data = f.read()
                            else:
                                csv_source.seek(0)
                                data = csv_source.read()

                            groups = parse_csv_groups_and_paths(data)
                            if not groups:
                                st.warning(f"Aucune donn√©e exploitable dans le CSV {csv_name}")
                                continue

                            total_files_in_csv = sum(len(paths) for paths in groups.values())
                            base_name = Path(csv_name).stem
                            db_path = os.path.join(base_root, base_name)
                            os.makedirs(db_path, exist_ok=True)
                            st.write(f"üìÇ CSV `{csv_name}` ‚Üí base `{base_name}` ({total_files_in_csv} fichiers)")
    
                            # Charger le CSV de tracking pour cette base
                            if base_name not in existing_entries_by_base:
                                existing_entries_by_base[base_name] = load_tracking_csv_for_base(base_name)
    
                            for group_name, paths in groups.items():
                                # V√©rifier si l'utilisateur a demand√© l'arr√™t
                                if check_stop():
                                    log("‚ö†Ô∏è Ingestion interrompue par l'utilisateur")
                                    ingestion_stopped = True
                                    break

                                progress(0.05, f"[{base_name}/{group_name}] Validation des chemins‚Ä¶")
                                new_paths: List[str] = []
                                missing_paths: List[str] = []
    
                                # Validation automatique des chemins / d√©tection fichiers manquants
                                for p in paths:
                                    if not os.path.exists(p):
                                        missing_paths.append(p)
                                        ingestion_stats["csv_missing_files"] += 1
                                        continue
                                    # Normaliser le chemin pour la comparaison (g√®re / vs \ et la casse sur Windows)
                                    p_normalized = os.path.normpath(p)
                                    key = (group_name, p_normalized)
                                    # V√©rifier aussi avec le chemin original au cas o√π
                                    key_original = (group_name, p)
                                    if key in existing_entries_by_base[base_name] or key_original in existing_entries_by_base[base_name]:
                                        ingestion_stats["csv_skipped_existing"] += 1
                                        continue
                                    new_paths.append(p)

                                if missing_paths:
                                    st.warning(
                                        f"‚ö†Ô∏è {len(missing_paths)} fichier(s) introuvable(s) pour "
                                        f"base={base_name}, group={group_name}. Voir les logs pour le d√©tail."
                                    )
                                    for mp in missing_paths:
                                        log(f"[MISSING] Fichier introuvable : {mp}")
    
                                # --- Extraction des pi√®ces jointes des PDF list√©s dans le CSV ---
                                temp_dirs_to_cleanup = []  # Liste des r√©pertoires temporaires √† nettoyer
                                if extract_attachments_from_pdf is not None:
                                    progress(0.10, f"[{base_name}/{group_name}] Analyse des pi√®ces jointes PDF‚Ä¶")
                                    original_pdf_paths = [p for p in new_paths if p.lower().endswith(".pdf")]
                                    for pdf_path in original_pdf_paths:
                                        try:
                                            # Utiliser un r√©pertoire temporaire pour les pi√®ces jointes
                                            info = extract_attachments_from_pdf(
                                                pdf_path,
                                                output_dir=None,  # Utilise tempfile automatiquement
                                            )
                                        except Exception as e:
                                            log(
                                                f"[ATTACH] Erreur lors de l'extraction des pi√®ces jointes de {pdf_path}: {e}"
                                            )
                                            info = {"attachments_paths": [], "pdf_without_attachments_path": None, "temp_dir": None}
    
                                        # Compatibilit√© : ancien retour (liste) ou nouveau (dict)
                                        if isinstance(info, dict):
                                            att_paths = info.get("attachments_paths", []) or []
                                            pdf_without = info.get("pdf_without_attachments_path")
                                            temp_dir = info.get("temp_dir")
                                            
                                            # Garder trace du r√©pertoire temporaire pour le nettoyer apr√®s l'ingestion
                                            if temp_dir:
                                                temp_dirs_to_cleanup.append(temp_dir)
                                        else:
                                            att_paths = info or []
                                            pdf_without = None
                                            temp_dir = None
    
                                        # Le PDF sans PJ est cr√©√© mais PAS ing√©r√© ET PAS ajout√© au CSV
                                        if pdf_without:
                                            log(
                                                f"[ATTACH-INFO] PDF sans PJ cr√©√© (non ing√©r√©, non trac√©) : {os.path.basename(pdf_without)}"
                                            )
    
                                        if att_paths:
                                            log(
                                                f"[ATTACH] {len(att_paths)} pi√®ce(s) jointe(s) d√©tect√©e(s) dans {pdf_path}"
                                            )
    
                                        for att_path in att_paths:
                                            att_name = os.path.basename(att_path)
                                            if not att_path:
                                                continue
    
                                            # Chemin logique pour le CSV, bas√© sur le fichier source
                                            csv_path_for_attachment = build_attachment_csv_path(pdf_path, att_name)
                                            key_attach = (group_name, csv_path_for_attachment)
                                            if key_attach in existing_entries_by_base[base_name]:
                                                log(
                                                    f"[ATTACH-SKIP] Pi√®ce jointe d√©j√† dans le CSV de tracking : "
                                                    f"base={base_name}, group={group_name}, path={csv_path_for_attachment}"
                                                )
                                                continue
    
                                            log(
                                                f"[ATTACH-ADD] Ajout pi√®ce jointe extraite : {att_name} "
                                                f"(base={base_name}, group={group_name})"
                                            )
                                            # Pour ingestion RAG, on utilise le chemin r√©el,
                                            # mais on associe un chemin logique pour le CSV / ref_id
                                            logical_paths[att_path] = csv_path_for_attachment
                                            new_paths.append(att_path)
                                            # Pour la tra√ßabilit√© CSV, on stocke le chemin logique demand√©
                                            ingested_entries.append((base_name, group_name, csv_path_for_attachment))
                                            existing_entries_by_base[base_name][key_attach] = True
                                            ingestion_stats["csv_attachments"] += 1
                                # --- Fin extraction pi√®ces jointes pour CSV ---
    
                                if not new_paths:
                                    log(
                                        f"[INGEST] Aucun nouveau fichier pour base={base_name}, "
                                        f"collection={group_name} (tout d√©j√† ing√©r√© ou introuvable)."
                                    )
                                    continue
    
                                ingestion_stats["csv_new_files"] += len(new_paths)
                                log(
                                    f"[INGEST] CSV {csv_name} ‚Üí base={base_name} collection={group_name} "
                                    f"({len(new_paths)} nouveau(x) fichier(s))"
                                )
                                # Ajout dans le r√©capitulatif uniquement des nouveaux fichiers
                                for p in new_paths:
                                    key2 = (group_name, p)
                                    if key2 not in existing_entries_by_base[base_name]:
                                        ingested_entries.append((base_name, group_name, p))
                                        existing_entries_by_base[base_name][key2] = True
    
                                # Appel ingestion avec callback de progression
                                progress(0.05, f"[{base_name}/{group_name}] D√©marrage de l'ingestion‚Ä¶")
                                try:
                                    # R√©cup√©rer les configs XML depuis session_state
                                    xml_configs_for_ingestion = st.session_state.get("xml_configs", {})

                                    report = ingest_documents(
                                        file_paths=new_paths,
                                        db_path=db_path,
                                        collection_name=group_name,
                                        chunk_size=chunk_size,
                                        use_easa_sections=use_easa_sections,
                                        rebuild=False,
                                        log=logger,
                                        logical_paths=logical_paths,
                                        progress_callback=progress,
                                        xml_configs=xml_configs_for_ingestion,
                                    )
                                    log(
                                        f"[INGEST] OK base={base_name}, collection={group_name}, "
                                        f"chunks={report.get('total_chunks', '?')}"
                                    )
    
                                    # Alimente le r√©capitulatif de session (ingestion via CSV)
                                    if isinstance(report, dict):
                                        for fi in report.get("files", []):
                                            session_ingested_files.append(
                                                {
                                                    "base": base_name,
                                                    "collection": group_name,
                                                    "file": fi.get("file", ""),
                                                    "chunks": fi.get("num_chunks", 0),
                                                    "language": fi.get("language", ""),
                                                    "sections": "oui" if fi.get("sections_detected") else "non",
                                                }
                                            )
                                except TypeError:
                                    # compat: ancienne signature sans mot-cl√©s
                                    report = ingest_documents(new_paths, db_path, group_name)
                                    log(
                                        f"[INGEST] (compat) OK base={base_name}, collection={group_name}"
                                    )
                                
                                # Nettoyage des r√©pertoires temporaires des pi√®ces jointes
                                for temp_dir in temp_dirs_to_cleanup:
                                    try:
                                        shutil.rmtree(temp_dir, ignore_errors=True)
                                        log(f"[CLEANUP] R√©pertoire temporaire pi√®ces jointes supprim√© : {temp_dir}")
                                    except Exception as e:
                                        log(f"[CLEANUP] √âchec suppression r√©pertoire temporaire {temp_dir} : {e}")

                                # Si arr√™t demand√©, sortir de la boucle des groupes
                                if ingestion_stopped:
                                    break

                            # Si arr√™t demand√©, sortir de la boucle des CSV
                            if ingestion_stopped:
                                break
                    finally:
                        try:
                            shutil.rmtree(csv_temp_dir, ignore_errors=True)
                            logger.info(f"[CLEANUP] R√©pertoire temporaire CSV supprim√© : {csv_temp_dir}")
                        except Exception as e:
                            logger.error(f"[CLEANUP] √âchec de suppression du r√©pertoire temporaire CSV {csv_temp_dir} : {e}")

                # ------------------------------------------------------------------
                # Fin : g√©n√©ration / mise √† jour CSV global + dashboard de synth√®se
                # ------------------------------------------------------------------
                if not csv_files_to_process:
                    st.warning("Aucun CSV d'ingestion s√©lectionn√© ou upload√©.")
                elif ingestion_stopped:
                    progress(1.0, "‚ö†Ô∏è Ingestion interrompue")
                    st.warning("‚ö†Ô∏è Ingestion interrompue par l'utilisateur. Les fichiers d√©j√† trait√©s ont √©t√© sauvegard√©s.")
                else:
                    progress(1.0, "‚úÖ Ingestion termin√©e.")
                    st.success("Ingestion termin√©e.")

                    # R√©initialiser les √©tats XML apr√®s ingestion r√©ussie
                    st.session_state["xml_preview_validated"] = False
                    st.session_state["xml_configs"] = {}
                    st.session_state["detected_xml_files"] = []

                    # R√©capitulatif des fichiers ing√©r√©s pour cette ex√©cution
                    if session_ingested_files:
                        st.markdown("### üìä R√©capitulatif des fichiers ing√©r√©s (cette ex√©cution)")
                        session_ingested_files_sorted = sorted(
                            session_ingested_files,
                            key=lambda x: (x.get("base", ""), x.get("collection", ""), x.get("file", "")),
                        )
                        st.table(session_ingested_files_sorted)
                    else:
                        st.info("Aucun nouveau fichier n'a √©t√© ing√©r√© pendant cette ex√©cution.")

                    # ------------------------------------------------------------------
                    # G√©n√©ration / mise √† jour des CSV de tracking par base
                    # ------------------------------------------------------------------
                    if existing_entries_by_base:
                        os.makedirs(CSV_EXPORT_DIR, exist_ok=True)

                        for base_name, entries_dict in existing_entries_by_base.items():
                            # Convertir le dict en liste de tuples (group, path)
                            entries_list = [(g, p) for (g, p) in entries_dict.keys()]

                            if entries_list:
                                output_buf = io.StringIO()
                                writer = csv.writer(output_buf, delimiter=";")
                                writer.writerow(["group", "path"])
                                for g, p in sorted(entries_list):
                                    writer.writerow([g, p])

                                csv_data = output_buf.getvalue().encode("utf-8-sig")
                                csv_path = get_tracking_csv_path(base_name)

                                if csv_path:
                                    try:
                                        with open(csv_path, "wb") as f_out:
                                            f_out.write(csv_data)
                                        log(f"CSV de tracking mis √† jour pour la base '{base_name}' : {csv_path}")
                                    except Exception as e:
                                        st.error(
                                            f"Impossible d'√©crire le CSV de tracking pour '{base_name}' : {e}"
                                        )

                        st.markdown("#### üìÑ CSV de tracking mis √† jour par base")
                        st.info(f"‚úÖ {len(existing_entries_by_base)} CSV de tracking mis √† jour (un par base)")
                    else:
                        st.info("Aucun document ing√©r√©, CSV de tracking non g√©n√©r√©.")

                    # ------------------------------------------------------------------
                    # R√©sum√© compact de l'ingestion (dashboard)
                    # ------------------------------------------------------------------
                    st.markdown("#### üìä R√©sum√© de l'ingestion")
                    c1, c2, c3, c4 = st.columns(4)
                    with c1:
                        st.metric("Nouveaux fichiers", ingestion_stats["csv_new_files"])
                    with c2:
                        st.metric("Fichiers manquants", ingestion_stats["csv_missing_files"])
                    with c3:
                        st.metric("D√©j√† pr√©sents (skipped)", ingestion_stats["csv_skipped_existing"])
                    with c4:
                        st.metric("Pi√®ces jointes", ingestion_stats["csv_attachments"])

            finally:
                # Supprimer tous les verrous cr√©√©s, m√™me en cas d'erreur
                for base_name in created_locks:
                    remove_ingestion_lock(base_root, base_name)
                    logger.info(f"Verrou d'ingestion supprim√© pour {base_name}")

                # R√©initialiser les flags d'ingestion et les verrous sauvegard√©s
                st.session_state["ingestion_running"] = False
                st.session_state["stop_ingestion"] = False
                st.session_state["created_locks"] = []

                # Masquer le bouton stop
                stop_button_placeholder.empty()


# ========================
#   TAB PURGE DES BASES
# ========================
with tab_purge:
    st.subheader("üóëÔ∏è Purge des bases FAISS")

    st.warning(
        "‚ö†Ô∏è **ATTENTION** : La purge d'une base supprimera **TOUT** le contenu de toutes ses collections "
        "et mettra √† jour le CSV de tracking correspondant. Cette action est irr√©versible !"
    )

    if not bases:
        st.info("‚ÑπÔ∏è Aucune base FAISS trouv√©e.")
    else:
        st.markdown("### S√©lection de la base √† purger")

        base_to_purge = st.selectbox(
            "Choisissez la base √† purger",
            options=bases,
            key="base_to_purge"
        )

        if base_to_purge:
            # Afficher les statistiques de la base
            st.markdown(f"### üìä Statistiques de la base `{base_to_purge}`")

            collections = list_collections_for_base(base_root, base_to_purge)
            col_counts = get_collection_doc_counts(base_root, base_to_purge)

            if collections:
                total_docs = sum(col_counts.values())

                col1, col2 = st.columns(2)
                with col1:
                    st.metric("Nombre de collections", len(collections))
                with col2:
                    st.metric("Total de chunks index√©s", total_docs)

                st.markdown("#### D√©tail par collection")
                collections_details = []
                for coll_name in collections:
                    collections_details.append({
                        "Collection": coll_name,
                        "Nombre de chunks": col_counts.get(coll_name, 0)
                    })
                st.table(collections_details)

                # V√©rifier si un CSV de tracking existe
                tracking_csv_path = get_tracking_csv_path(base_to_purge)
                csv_exists = tracking_csv_path and os.path.exists(tracking_csv_path)

                if csv_exists:
                    st.info(f"üìÑ Un CSV de tracking existe pour cette base : `{os.path.basename(tracking_csv_path)}`")
                else:
                    st.info("‚ÑπÔ∏è Aucun CSV de tracking trouv√© pour cette base.")

                # Section de confirmation
                st.markdown("---")
                st.markdown("### ‚ö†Ô∏è Confirmation de purge")

                confirm_text = st.text_input(
                    f"Pour confirmer la purge, tapez le nom de la base : **{base_to_purge}**",
                    key="confirm_purge_text"
                )

                if st.button("üóëÔ∏è PURGER LA BASE", type="primary", disabled=(confirm_text != base_to_purge), help="Supprime d√©finitivement tout le contenu de la base s√©lectionn√©e. Cette action est irr√©versible !"):
                    if confirm_text == base_to_purge:
                        progress_bar = st.progress(0.0)
                        status_text = st.empty()

                        try:
                            db_path = os.path.join(base_root, base_to_purge)
                            store = build_faiss_store(db_path)

                            # Purger chaque collection (FAISS: suppression compl√®te de la collection)
                            status_text.info(f"Purge de {len(collections)} collection(s) en cours...")
                            purged_collections = []

                            for idx, coll_name in enumerate(collections):
                                try:
                                    # Compter les √©l√©ments avant suppression
                                    try:
                                        collection = store.get_collection(coll_name)
                                        doc_count = collection.count()
                                    except:
                                        doc_count = 0

                                    # FAISS: supprimer toute la collection
                                    store.delete_collection(coll_name)

                                    logger.info(f"[PURGE] Collection '{coll_name}' purg√©e : {doc_count} chunks supprim√©s")
                                    purged_collections.append((coll_name, doc_count))

                                    progress_bar.progress((idx + 1) / len(collections))

                                except Exception as e:
                                    st.error(f"‚ùå Erreur lors de la purge de la collection '{coll_name}' : {e}")
                                    logger.error(f"[PURGE] Erreur collection {coll_name}: {e}")

                            # Supprimer le CSV de tracking
                            if csv_exists:
                                try:
                                    os.remove(tracking_csv_path)
                                    st.success(f"‚úÖ CSV de tracking supprim√© : {os.path.basename(tracking_csv_path)}")
                                    logger.info(f"[PURGE] CSV de tracking supprim√© : {tracking_csv_path}")
                                except Exception as e:
                                    st.warning(f"‚ö†Ô∏è Impossible de supprimer le CSV de tracking : {e}")
                                    logger.error(f"[PURGE] Erreur suppression CSV : {e}")

                            progress_bar.progress(1.0)
                            status_text.empty()

                            # Afficher le r√©sum√©
                            st.success(f"‚úÖ Base `{base_to_purge}` purg√©e avec succ√®s !")

                            total_purged = sum(count for _, count in purged_collections)
                            st.markdown(f"**Total de chunks supprim√©s** : {total_purged}")

                            st.markdown("#### D√©tail de la purge")
                            purge_details = []
                            for coll_name, count in purged_collections:
                                purge_details.append({
                                    "Collection": coll_name,
                                    "Chunks supprim√©s": count
                                })
                            st.table(purge_details)

                            st.info("üîÑ Rechargez la page pour voir les changements dans la sidebar.")

                        except Exception as e:
                            st.error(f"‚ùå Erreur lors de la purge de la base : {e}")
                            logger.error(f"[PURGE] Erreur globale : {e}")
                    else:
                        st.warning("‚ö†Ô∏è Le nom de la base ne correspond pas. Purge annul√©e.")
            else:
                st.info("‚ÑπÔ∏è Cette base ne contient aucune collection.")


# ========================
#   Helpers affichage RAG
# ========================

def open_file_callback(file_path: str):
    """Callback pour ouvrir un fichier sans recharger la page Streamlit"""
    try:
        import subprocess
        import platform

        if platform.system() == "Windows":
            os.startfile(file_path)
        elif platform.system() == "Darwin":  # macOS
            subprocess.Popen(["open", file_path])
        else:  # Linux
            subprocess.Popen(["xdg-open", file_path])
    except Exception as e:
        # Stocker l'erreur dans session_state pour l'afficher
        st.session_state['file_open_error'] = f"Impossible d'ouvrir le fichier : {e}"


def is_llm_error_response(answer: str) -> bool:
    """V√©rifie si la r√©ponse est un message d'erreur du LLM."""
    if not answer:
        return False
    error_markers = [
        "ERREUR DE COMMUNICATION AVEC LE LLM",
        "Le serveur n'a pas pu r√©pondre",
        "Veuillez reposer votre question",
    ]
    return any(marker in answer for marker in error_markers)


def display_answer_with_error_check(answer: str) -> None:
    """Affiche la r√©ponse avec d√©tection d'erreur et alerte."""
    if not answer or not answer.strip():
        st.warning("‚ö†Ô∏è Aucune r√©ponse g√©n√©r√©e par le LLM. V√©rifiez la configuration des mod√®les ou les logs.")
    elif is_llm_error_response(answer):
        st.error(answer)
        st.info("üí° **Conseil**: Attendez quelques secondes puis cliquez sur 'Rechercher' √† nouveau.")
    else:
        st.write(answer)


def render_sources_list(
    sources: List[Dict],
    global_title: str = "Sources utilis√©es (tri√©es par pertinence)",
    show_collection: bool = False,
) -> None:
    """Affichage compact et color√© des sources RAG avec contexte par chunk."""
    if not sources:
        return

    # Limiter √† 10 meilleurs candidats
    sources = sources[:10]

    st.markdown(f"### {global_title}")
    for i, src in enumerate(sources, start=1):
        score = float(src.get("score", 0.0) or 0.0)
        distance = float(src.get("distance", 0.0) or 0.0)

        # Code couleur avec seuils ajust√©s (plus permissifs)
        # Vert: score >= 0.5, Orange: score >= 0.3, Rouge: score < 0.3
        if score >= 0.5:
            badge = "üü¢"
        elif score >= 0.3:
            badge = "üü†"
        else:
            badge = "üî¥"

        collection_label = ""
        if show_collection:
            collection_label = f"[{src.get('collection', '?')}] "

        # Nom du document (source_file)
        doc_name = src.get('source_file', 'unknown')

        header = (
            f"{badge} {i}. {collection_label}"
            f"{doc_name} "
            f"(score {score:.3f})"
        )

        with st.expander(header):
            # Afficher le chemin complet du fichier
            file_path = src.get("path", "")
            if file_path:
                st.markdown(f"**Source** : `{file_path}`")

                # Bouton pour ouvrir le fichier (utilise un callback pour √©viter le rerun)
                # Utiliser un hash du chemin complet pour garantir l'unicit√©
                file_hash = hashlib.md5(file_path.encode()).hexdigest()[:8]
                st.button(
                    f"üìÇ Ouvrir",
                    key=f"open_{i}_{file_hash}",
                    on_click=open_file_callback,
                    args=(file_path,)
                )

            section_id = src.get("section_id") or ""
            section_title = src.get("section_title") or ""
            if section_id or section_title:
                st.markdown("**Section EASA d√©tect√©e :**")
                st.write(f"{section_id} {section_title}".strip())

            st.markdown("**Passage utilis√© (chunk) :**")
            st.write(src.get("text", ""))


# ========================
#   TAB RAG
# ========================
with tab_rag:
    st.subheader("‚ùì Poser une question au RAG (DALLEM)")

    # S√©lection de la base et collection
    sel_col1, sel_col2 = st.columns(2)

    with sel_col1:
        st.markdown("**Base s√©lectionn√©e :**")
        if bases:
            base_for_query = st.selectbox(
                "Base pour la recherche",
                options=bases,
                label_visibility="collapsed"
            )
        else:
            base_for_query = st.selectbox(
                "Base pour la recherche",
                options=["(aucune)"],
                label_visibility="collapsed"
            )

    with sel_col2:
        st.markdown("**Collection :**")
        # Obtenir les collections pour la base s√©lectionn√©e
        if bases and base_for_query != "(aucune)":
            collections_for_query = list_collections_for_base(base_root, base_for_query)
            col_counts = get_collection_doc_counts(base_root, base_for_query)
        else:
            collections_for_query = []
            col_counts = {}

        collection_for_query = st.selectbox(
            "Collection pour la recherche",
            options=(["ALL"] + collections_for_query) if collections_for_query else ["ALL"],
            label_visibility="collapsed"
        )

    st.markdown("---")

    question = st.text_area("Question", height=110, placeholder="Saisissez votre question m√©tier‚Ä¶")

    top_k = 30  # Recherche 30 candidats, affiche les 10 meilleurs

    synthesize_all = st.checkbox(
        "R√©ponse synth√©tis√©e unique si 'ALL' est s√©lectionn√©",
        value=True,
        help=(
            "Si coch√© et que la collection choisie est 'ALL', le LLM produira une "
            "seule r√©ponse globale en utilisant le contexte de toutes les collections. "
            "Si d√©coch√©, une r√©ponse sera produite pour chaque collection."
        ),
    )

    # Option de re-ranking bas√© sur les feedbacks
    use_feedback_reranking = st.checkbox(
        "üîÑ Utiliser les retours utilisateurs pour am√©liorer les r√©sultats",
        value=True,
        help=(
            "Si activ√©, les sources sont re-class√©es en fonction des feedbacks "
            "pr√©c√©dents : les sources bien not√©es sont favoris√©es, celles mal "
            "not√©es sont p√©nalis√©es. Les questions similaires pass√©es sont "
            "√©galement prises en compte."
        ),
    )

    # Bouton pour poser la question
    if st.button("ü§ñ Poser la question", help="Recherche les documents pertinents et g√©n√®re une r√©ponse via DALLEM bas√©e sur le contexte trouv√©."):
        if not question.strip():
            st.warning("Merci de saisir une question.")
        else:
            if not bases or base_for_query == "(aucune)":
                st.error("Aucune base valide s√©lectionn√©e pour la recherche.")
            else:
                db_path_query = os.path.join(base_root, base_for_query)
                os.makedirs(db_path_query, exist_ok=True)

                try:
                    # R√©initialiser le flag de feedback pour permettre un nouveau feedback
                    st.session_state['feedback_submitted'] = False

                    # Stocker les param√®tres de la recherche
                    st.session_state['last_search'] = {
                        'base': base_for_query,
                        'collection': collection_for_query,
                        'question': question,
                        'synthesize_all': synthesize_all,
                        'top_k': top_k
                    }

                    if collection_for_query != "ALL":
                        # RAG sur une collection unique
                        spinner_msg = "üîç RAG en cours‚Ä¶"
                        with st.spinner(spinner_msg):
                            # Utiliser le cache pour les requ√™tes r√©p√©t√©es (30 min TTL)
                            result = cached_rag_query(
                                db_path=db_path_query,
                                collection_name=collection_for_query,
                                question=question,
                                top_k=top_k,
                                use_feedback_reranking=use_feedback_reranking,
                                use_query_expansion=True,
                                use_bge_reranker=True,
                            )

                        # Stocker le r√©sultat dans session_state
                        st.session_state['last_result'] = result
                        st.session_state['result_type'] = 'single'

                    else:
                        # RAG sur TOUTES les collections de la base
                        collections_all = list_collections_for_base(base_root, base_for_query)
                        if not collections_all:
                            st.warning("Aucune collection trouv√©e dans cette base.")
                        else:
                            if synthesize_all:
                                st.info(
                                    f"RAG synth√©tis√© sur toutes les collections de la base `{base_for_query}`."
                                )
                                spinner_msg = "üîç RAG en cours (ALL)‚Ä¶"
                                with st.spinner(spinner_msg):
                                    # Utiliser le cache pour les requ√™tes r√©p√©t√©es (30 min TTL)
                                    result = cached_rag_query(
                                        db_path=db_path_query,
                                        collection_name="ALL",
                                        question=question,
                                        top_k=top_k,
                                        synthesize_all=True,
                                        use_feedback_reranking=use_feedback_reranking,
                                        use_query_expansion=True,
                                        use_bge_reranker=True,
                                    )

                                # Stocker le r√©sultat dans session_state
                                st.session_state['last_result'] = result
                                st.session_state['result_type'] = 'synthesized'

                            else:
                                st.info(
                                    f"RAG sur toutes les collections de la base `{base_for_query}` : "
                                    + ", ".join(collections_all)
                                )
                                all_results = []
                                spinner_msg = "üîç RAG en cours (toutes collections)‚Ä¶"
                                with st.spinner(spinner_msg):
                                    for coll in collections_all:
                                        try:
                                            # Utiliser le cache pour les requ√™tes r√©p√©t√©es (30 min TTL)
                                            res = cached_rag_query(
                                                db_path=db_path_query,
                                                collection_name=coll,
                                                question=question,
                                                top_k=top_k,
                                                use_feedback_reranking=use_feedback_reranking,
                                                use_query_expansion=True,
                                                use_bge_reranker=True,
                                            )
                                            all_results.append((coll, res))
                                        except Exception as e:
                                            st.error(f"Erreur sur la collection {coll}: {e}")

                                # Stocker les r√©sultats dans session_state
                                st.session_state['last_result'] = all_results
                                st.session_state['result_type'] = 'individual'

                except Exception as e:  # pragma: no cover
                    import traceback
                    st.error(f"‚ùå Erreur pendant l'appel RAG : {e}")
                    st.code(traceback.format_exc())

    # Afficher les r√©sultats depuis session_state (persiste m√™me apr√®s un rerun)
    if 'last_result' in st.session_state and 'last_search' in st.session_state:
        search_params = st.session_state['last_search']
        result_type = st.session_state.get('result_type', 'single')

        # Afficher l'erreur d'ouverture de fichier si elle existe
        if 'file_open_error' in st.session_state:
            st.error(st.session_state['file_open_error'])
            del st.session_state['file_open_error']

        if result_type == 'single':
            # Affichage pour une collection unique
            result = st.session_state['last_result']
            st.markdown(
                f"### üß† R√©ponse (base=`{search_params['base']}`, collection=`{search_params['collection']}`)"
            )
            answer = result.get("answer", "")
            display_answer_with_error_check(answer)

            sources = result.get("sources", [])
            sources = sorted(
                sources, key=lambda s: s.get("score", 0.0), reverse=True
            )

            if sources:
                render_sources_list(
                    sources,
                    global_title="üìö Sources utilis√©es (tri√©es par pertinence)",
                    show_collection=False,
                )

            with st.expander("üß© Voir le contexte brut (concat√©n√© ‚Äì debug)"):
                st.text(result.get("context_str", ""))

        elif result_type == 'synthesized':
            # Affichage pour ALL synth√©tis√©
            result = st.session_state['last_result']
            st.markdown(
                f"### üß† R√©ponse synth√©tis√©e (base=`{search_params['base']}`, collections=ALL)"
            )
            answer = result.get("answer", "")
            display_answer_with_error_check(answer)

            sources = result.get("sources", [])
            sources = sorted(
                sources, key=lambda s: s.get("score", 0.0), reverse=True
            )

            if sources:
                render_sources_list(
                    sources,
                    global_title=(
                        "üìö Sources utilis√©es (toutes collections, tri√©es par pertinence)"
                    ),
                    show_collection=True,
                )

            with st.expander("üß© Voir le contexte brut (concat√©n√© ‚Äì debug)"):
                st.text(result.get("context_str", ""))

        elif result_type == 'individual':
            # Affichage pour ALL individuel
            all_results = st.session_state['last_result']
            for coll, result in all_results:
                st.markdown(f"### üß† R√©ponse ‚Äì Collection `{coll}`")
                answer = result.get("answer", "")
                display_answer_with_error_check(answer)

                sources = result.get("sources", [])
                sources = sorted(
                    sources, key=lambda s: s.get("score", 0.0), reverse=True
                )

                if sources:
                    render_sources_list(
                        sources,
                        global_title=(
                            f"üìö Sources utilis√©es (collection `{coll}`, tri√©es par pertinence)"
                        ),
                        show_collection=False,
                    )

                with st.expander(
                    f"üß© Contexte brut pour la collection `{coll}` (debug)"
                ):
                    st.text(result.get("context_str", ""))

        # ========================
        #   SECTION FEEDBACK (SIMPLIFI√â)
        # ========================
        st.markdown("---")
        st.markdown("### üìù Cette r√©ponse vous a-t-elle aid√© ?")

        # Initialiser l'√©tat du feedback si n√©cessaire
        if 'feedback_submitted' not in st.session_state:
            st.session_state['feedback_submitted'] = False

        if not st.session_state['feedback_submitted']:
            # Boutons pouce haut / pouce bas
            col_thumbs1, col_thumbs2, col_space = st.columns([1, 1, 3])

            with col_thumbs1:
                thumbs_up = st.button("üëç Oui", type="primary", use_container_width=True, help="La r√©ponse est utile et pertinente")

            with col_thumbs2:
                thumbs_down = st.button("üëé Non", use_container_width=True, help="La r√©ponse n'est pas satisfaisante")

            # Champ pour la r√©ponse attendue (affich√© seulement si pouce bas)
            if 'show_suggested_answer' not in st.session_state:
                st.session_state['show_suggested_answer'] = False

            if thumbs_up:
                try:
                    # R√©cup√©rer le texte de la r√©ponse
                    if result_type == 'individual':
                        answer_text = "\n---\n".join([
                            f"[{coll}] {res.get('answer', '')}"
                            for coll, res in st.session_state.get('last_result', [])
                        ])
                    else:
                        answer_text = st.session_state.get('last_result', {}).get('answer', '')

                    # Cr√©er le feedback positif
                    feedback = create_feedback(
                        base_name=search_params['base'],
                        collection_name=search_params['collection'],
                        question=search_params['question'],
                        is_positive=True,
                        answer_text=answer_text,
                        top_k_used=search_params.get('top_k', 10),
                        synthesize_all=search_params.get('synthesize_all', False)
                    )

                    # Sauvegarder
                    feedback_id = feedback_store.add_feedback(feedback)
                    st.session_state['feedback_submitted'] = True
                    st.session_state['last_feedback_id'] = feedback_id
                    st.session_state['feedback_positive'] = True
                    st.rerun()

                except Exception as e:
                    st.error(f"Erreur lors de l'enregistrement du feedback : {e}")

            if thumbs_down:
                st.session_state['show_suggested_answer'] = True
                st.rerun()

            # Formulaire pour la r√©ponse attendue (si pouce bas)
            if st.session_state.get('show_suggested_answer', False):
                st.markdown("---")
                st.markdown("üí° **Aidez-nous √† am√©liorer !** Quelle r√©ponse auriez-vous attendue ?")

                suggested_answer = st.text_area(
                    "R√©ponse attendue",
                    placeholder="D√©crivez la r√©ponse que vous attendiez...",
                    height=100,
                    label_visibility="collapsed"
                )

                col_send, col_cancel = st.columns([1, 1])

                with col_send:
                    if st.button("üì§ Envoyer", type="primary", use_container_width=True):
                        try:
                            # R√©cup√©rer le texte de la r√©ponse
                            if result_type == 'individual':
                                answer_text = "\n---\n".join([
                                    f"[{coll}] {res.get('answer', '')}"
                                    for coll, res in st.session_state.get('last_result', [])
                                ])
                            else:
                                answer_text = st.session_state.get('last_result', {}).get('answer', '')

                            # Cr√©er le feedback n√©gatif avec suggestion
                            feedback = create_feedback(
                                base_name=search_params['base'],
                                collection_name=search_params['collection'],
                                question=search_params['question'],
                                is_positive=False,
                                suggested_answer=suggested_answer,
                                answer_text=answer_text,
                                top_k_used=search_params.get('top_k', 10),
                                synthesize_all=search_params.get('synthesize_all', False)
                            )

                            # Sauvegarder
                            feedback_id = feedback_store.add_feedback(feedback)
                            st.session_state['feedback_submitted'] = True
                            st.session_state['last_feedback_id'] = feedback_id
                            st.session_state['feedback_positive'] = False
                            st.session_state['show_suggested_answer'] = False
                            st.rerun()

                        except Exception as e:
                            st.error(f"Erreur lors de l'enregistrement du feedback : {e}")

                with col_cancel:
                    if st.button("‚ùå Annuler", use_container_width=True):
                        st.session_state['show_suggested_answer'] = False
                        st.rerun()

        else:
            if st.session_state.get('feedback_positive', True):
                st.success("‚úÖ Merci pour votre retour positif !")
            else:
                st.success("‚úÖ Merci ! Votre suggestion sera prise en compte.")

            if st.button("üîÑ Nouvelle question", help="Poser une nouvelle question"):
                st.session_state['feedback_submitted'] = False
                st.session_state['show_suggested_answer'] = False
                if 'last_result' in st.session_state:
                    del st.session_state['last_result']
                if 'last_search' in st.session_state:
                    del st.session_state['last_search']
                st.rerun()


# ========================
#   TAB TABLEAU DE BORD ANALYTIQUE (SIMPLIFI√â)
#   Accessible uniquement aux utilisateurs autoris√©s
# ========================
if tab_analytics is not None:
    with tab_analytics:
        st.subheader("üìä Tableau de bord - Retours utilisateurs")

        # S√©lection de la base √† analyser
        st.markdown("### Filtres")

        col_filter1, col_filter2 = st.columns(2)

        with col_filter1:
            analytics_base = st.selectbox(
                "Base √† analyser",
                options=["Toutes les bases"] + bases,
                key="analytics_base"
            )

        with col_filter2:
            analytics_period = st.selectbox(
                "P√©riode d'analyse",
                options=[7, 14, 30, 60, 90],
                format_func=lambda x: f"Derniers {x} jours",
                index=2,
                key="analytics_period"
            )

        # R√©cup√©rer les statistiques
        base_filter = None if analytics_base == "Toutes les bases" else analytics_base

        try:
            stats = feedback_store.get_statistics(base_filter)
            trends = feedback_store.get_feedback_trends(base_filter, days=analytics_period)
        except Exception as e:
            st.error(f"Erreur lors du chargement des statistiques : {e}")
            stats = {"total_feedbacks": 0}
            trends = {"total_feedbacks_in_period": 0, "trend_data": []}

        # M√©triques principales
        st.markdown("### M√©triques globales")

        if stats["total_feedbacks"] > 0:
            col_m1, col_m2, col_m3 = st.columns(3)

            with col_m1:
                st.metric(
                    "Total feedbacks",
                    stats["total_feedbacks"],
                    help="Nombre total de feedbacks enregistr√©s"
                )

            with col_m2:
                positive = stats.get("positive_feedbacks", 0)
                negative = stats.get("negative_feedbacks", 0)
                st.metric(
                    "üëç Positifs",
                    positive,
                    help="Nombre de r√©ponses jug√©es utiles"
                )

            with col_m3:
                satisfaction_rate = stats.get("satisfaction_rate", 0)
                st.metric(
                    "Taux de satisfaction",
                    f"{satisfaction_rate}%",
                    help="Pourcentage de feedbacks positifs"
                )

            # Graphiques de tendance
            st.markdown("### √âvolution des feedbacks")

            trend_data = trends.get("trend_data", [])
            if trend_data:
                import pandas as pd

                df_trends = pd.DataFrame(trend_data)
                df_trends["date"] = pd.to_datetime(df_trends["date"])

                # Graphique empil√© positif/n√©gatif
                st.bar_chart(
                    df_trends.set_index("date")[["positive", "negative"]],
                    use_container_width=True
                )
                st.caption("Feedbacks positifs (üëç) et n√©gatifs (üëé) par jour")

            else:
                st.info(f"Pas de donn√©es de tendance pour les {analytics_period} derniers jours.")

            # Statistiques par collection
            st.markdown("### Satisfaction par collection")

            collection_stats = stats.get("collection_stats", {})
            if collection_stats:
                coll_data = []
                for coll_name, coll_stats in collection_stats.items():
                    coll_data.append({
                        "Collection": coll_name,
                        "Total": coll_stats.get("count", 0),
                        "üëç": coll_stats.get("positive", 0),
                        "üëé": coll_stats.get("negative", 0),
                        "Satisfaction": f"{coll_stats.get('satisfaction_rate', 0)}%"
                    })

                # Trier par satisfaction d√©croissante
                coll_data.sort(key=lambda x: float(x["Satisfaction"].replace("%", "")), reverse=True)
                st.table(coll_data)
            else:
                st.info("Aucune statistique par collection disponible.")

            # Questions avec feedback n√©gatif
            st.markdown("### Questions avec feedback n√©gatif")

            negative_questions = stats.get("negative_questions", [])
            if negative_questions:
                st.warning(f"{len(negative_questions)} question(s) avec feedback n√©gatif")

                for idx, q in enumerate(negative_questions[:10], 1):
                    question_text = q.get('question', 'N/A')
                    question_preview = question_text[:80] + "..." if len(question_text) > 80 else question_text

                    with st.expander(f"üëé {question_preview}"):
                        st.write(f"**Question compl√®te**: {question_text}")
                        st.write(f"**Base**: {q.get('base', 'N/A')}")
                        st.write(f"**Collection**: {q.get('collection', 'N/A')}")
                        st.write(f"**Date**: {q.get('timestamp', 'N/A')[:10] if q.get('timestamp') else 'N/A'}")
                        if q.get('suggested_answer'):
                            st.markdown("**üí° R√©ponse attendue par l'utilisateur:**")
                            st.info(q.get('suggested_answer'))
            else:
                st.success("Aucune question avec feedback n√©gatif !")

            # Statistiques utilisateurs
            st.markdown("### Activit√© par utilisateur")

            user_stats = stats.get("user_stats", {})
            if user_stats:
                user_data = [
                    {"Utilisateur": user, "Feedbacks": count}
                    for user, count in sorted(user_stats.items(), key=lambda x: x[1], reverse=True)
                ]
                st.table(user_data[:10])
            else:
                st.info("Aucune statistique utilisateur disponible.")

            # Export des donn√©es
            st.markdown("---")
            st.markdown("### Export des donn√©es")

            col_export1, col_export2 = st.columns(2)

            with col_export1:
                if st.button("üì• Exporter en CSV", use_container_width=True, help="G√©n√®re un fichier CSV t√©l√©chargeable contenant tous les feedbacks enregistr√©s."):
                    try:
                        csv_content = feedback_store.export_feedbacks_csv(base_filter)
                        st.download_button(
                            label="üíæ T√©l√©charger le CSV",
                            data=csv_content.encode("utf-8-sig"),
                            file_name=f"feedbacks_{analytics_base.replace(' ', '_')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                            mime="text/csv",
                            use_container_width=True
                        )
                    except Exception as e:
                        st.error(f"Erreur lors de l'export : {e}")

            with col_export2:
                if st.button("üìä Rafra√Æchir les statistiques", use_container_width=True, help="Recharge les donn√©es et recalcule les statistiques du tableau de bord."):
                    st.rerun()

        else:
            st.info("Aucun feedback enregistr√© pour le moment.")
            st.markdown("""
            **Comment collecter des feedbacks ?**

            1. Posez une question dans l'onglet "‚ùì Questions RAG"
            2. Apr√®s avoir re√ßu une r√©ponse, cliquez sur üëç ou üëé
            3. Si vous cliquez üëé, vous pouvez indiquer la r√©ponse attendue
            4. Les statistiques appara√Ætront ici automatiquement
            """)
